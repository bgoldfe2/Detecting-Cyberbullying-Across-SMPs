{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code forked from Sweta Agrawal et al. paper 2018\n",
    "\n",
    "import sys\n",
    "# Changed to add python 3 BHG\n",
    "sys.path.append('/home/bruce/anaconda3/envs/conda_clone/bin/python')\n",
    "# adjust warnings as needed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "\n",
    "import numpy as np\n",
    "import os, sys, getopt, pickle, csv, sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, make_scorer, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import argparse\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score    \n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of Traditional Models using SVM, Naive Bayes, Linear Regression, and Random Forest\n",
    "\n",
    "models = [ 'svm', 'naive', 'lr', 'random_forest']\n",
    "NO_OF_FOLDS = 10\n",
    "MODEL_TYPE = \"all\"\n",
    "HASH_REMOVE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        if(HASH_REMOVE):\n",
    "            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
    "        else:\n",
    "            x_text.append(data[i]['text'])\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels\n",
    "\n",
    "def get_filename(dataset):\n",
    "    global N_CLASS, HASH_REMOVE\n",
    "    if(dataset==\"twitter\"):\n",
    "        filename = \"data/twitter_data.pkl\"\n",
    "        N_CLASS = 3\n",
    "        HASH_REMOVE = False\n",
    "    elif(dataset==\"formspring\"):\n",
    "        N_CLASS = 2\n",
    "        filename = \"data/formspring_data.pkl\"\n",
    "        HASH_REMOVE = False\n",
    "    elif(dataset==\"wiki\"):\n",
    "        N_CLASS = 2\n",
    "        filename = \"data/wiki_data.pkl\"\n",
    "        HASH_REMOVE = False\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_true, y_pred):\n",
    "#     if(data==\"wiki\"):\n",
    "#         auc = roc_auc_score(y_true,y_pred)\n",
    "#         print('Test ROC AUC: %.3f' %auc)\n",
    "#     print(\":: Confusion Matrix\")\n",
    "#     print(confusion_matrix(y_true, y_pred))\n",
    "#     print(\":: Classification Report\")\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "    return np.array([ \n",
    "            precision_score(y_true, y_pred, average=None), \n",
    "            recall_score(y_true, y_pred,  average=None),\n",
    "            f1_score(y_true, y_pred, average=None)])\n",
    "    \n",
    "def print_scores(scores):\n",
    "    for i in range(N_CLASS):\n",
    "        if(i!=0):\n",
    "            # Changed print syntax from py2 to py3 by adding parentheses BHG\n",
    "            print (\"Precision Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:, i].mean(), scores[:, i].std() * 2))\n",
    "            print (\"Recall Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:,  N_CLASS+i].mean(), scores[:,N_CLASS+i].std() * 2))\n",
    "            print (\"F1_score Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:, N_CLASS*2+i].mean(), scores[:,  N_CLASS*2+i].std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(X, Y, model_type):\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    # Changed print syntax from py2 to py3 by adding parentheses BHG\n",
    "    print (\"Model Type:\", model_type)\n",
    "    kf = KFold(n_splits=NO_OF_FOLDS)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        Y = np.asarray(Y)\n",
    "        model = get_model(model_type)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        curr_scores = get_scores(y_test, y_pred)\n",
    "        scores.append(np.hstack(curr_scores))\n",
    "    print_scores(np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(m_type):\n",
    "    if m_type == 'lr':\n",
    "        # Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "        logreg = LogisticRegression(class_weight=\"balanced\")\n",
    "    elif m_type == 'naive':\n",
    "        # Naive Bayes classifier for multinomial models\n",
    "        logreg =  MultinomialNB()\n",
    "    elif m_type == \"random_forest\":\n",
    "        # A random forest classifier\n",
    "        logreg = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "    elif m_type == \"svm\":\n",
    "        # Linear Support Vector Classification\n",
    "        logreg = LinearSVC(class_weight=\"balanced\")\n",
    "    else:\n",
    "        print (\"ERROR: Please specify a correst model\")\n",
    "        return None\n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_text, labels, MODEL_TYPE):\n",
    "    \n",
    "    if(WORD):\n",
    "        print(\"Using word based features\")\n",
    "        bow_transformer = CountVectorizer(analyzer=\"word\",max_features = 10000,stop_words='english').fit(x_text)\n",
    "        comments_bow = bow_transformer.transform(x_text)\n",
    "        tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
    "        comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
    "        features = comments_tfidf\n",
    "    else: \n",
    "        print(\"Using char n-grams based features\")\n",
    "        bow_transformer = CountVectorizer(max_features = 10000, ngram_range = (1,2)).fit(x_text)\n",
    "        comments_bow = bow_transformer.transform(x_text)\n",
    "        tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
    "        comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
    "        features = comments_tfidf\n",
    "    \n",
    "    if(data == \"twitter\"):\n",
    "        dict1 = {'racism':0,'sexism':1,'none':2}\n",
    "        labels = np.array([dict1[b] for b in labels])\n",
    "    \n",
    "    from collections import Counter\n",
    "    print(Counter(labels))\n",
    "    \n",
    "    if(MODEL_TYPE != \"all\"):\n",
    "        classification_model(features, labels, MODEL_TYPE)\n",
    "    else:\n",
    "        for model_type in models:\n",
    "            classification_model(features, labels, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "\n",
      "Raw Data\n",
      "\n",
      " what is your favorite song ? d i like too many songs to have a favorite \n",
      "\n",
      "First 100 Labels - 0 = neutral, 1 = bullying\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1a - Formspring data set using Bag of Words\n",
    "data = \"formspring\"\n",
    "WORD =  False\n",
    "\n",
    "# Load the data set\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\\n\")\n",
    "\n",
    "# Examine the data\n",
    "print(\"Raw Data\\n\\n\",x_text[0],\"\\n\")\n",
    "\n",
    "#Examine the labels\n",
    "print(\"First 100 Labels - 0 = neutral, 1 = bullying\\n\\n\",np.array(labels[0:99]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using char n-grams based features\n",
      "Counter({0: 11997, 1: 776})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.466 (+/- 0.109)\n",
      "Recall Class 1 (avg): 0.503 (+/- 0.122)\n",
      "F1_score Class 1 (avg): 0.483 (+/- 0.104)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.850 (+/- 0.640)\n",
      "Recall Class 1 (avg): 0.015 (+/- 0.015)\n",
      "F1_score Class 1 (avg): 0.030 (+/- 0.028)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.410 (+/- 0.099)\n",
      "Recall Class 1 (avg): 0.626 (+/- 0.131)\n",
      "F1_score Class 1 (avg): 0.495 (+/- 0.104)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.782 (+/- 0.194)\n",
      "Recall Class 1 (avg): 0.171 (+/- 0.067)\n",
      "F1_score Class 1 (avg): 0.279 (+/- 0.092)\n"
     ]
    }
   ],
   "source": [
    "# Run Experiment 1a - ['Formspring','BOW'] - currently set to all models\n",
    "\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Raw Data\n",
      "\n",
      " what is your favorite song ? d i like too many songs to have a favorite \n",
      "\n",
      "Labels 0 = neutral, 1 = bullying\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1b - Formspring data set using n-grams\n",
    "data = \"formspring\"\n",
    "WORD = True\n",
    "\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "\n",
    "# Examine the data\n",
    "print(\"Raw Data\\n\\n\",x_text[0],\"\\n\")\n",
    "\n",
    "#Examine the labels\n",
    "print(\"Labels 0 = neutral, 1 = bullying\\n\\n\",np.array(labels[0:100]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using word based features\n",
      "Counter({0: 11997, 1: 776})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.415 (+/- 0.089)\n",
      "Recall Class 1 (avg): 0.525 (+/- 0.132)\n",
      "F1_score Class 1 (avg): 0.463 (+/- 0.100)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.575 (+/- 0.950)\n",
      "Recall Class 1 (avg): 0.013 (+/- 0.029)\n",
      "F1_score Class 1 (avg): 0.025 (+/- 0.055)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.407 (+/- 0.079)\n",
      "Recall Class 1 (avg): 0.617 (+/- 0.127)\n",
      "F1_score Class 1 (avg): 0.489 (+/- 0.084)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.679 (+/- 0.226)\n",
      "Recall Class 1 (avg): 0.163 (+/- 0.066)\n",
      "F1_score Class 1 (avg): 0.262 (+/- 0.099)\n"
     ]
    }
   ],
   "source": [
    "# Run Experiment 1b - ['Formspring','n-grams'] - currently set to all models\n",
    "\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Labels ['sexism','racism','none']\n",
      "\n",
      " ['none' 'none' 'none' 'none' 'none' 'none' 'racism' 'racism' 'sexism'] \n",
      "\n",
      "None example\n",
      "\n",
      " rt @colonelkickhead: another bloody instant restaurant week ?  !  ?  !  seriously !  they just jumped the shark riding two other sharks powered by sh… \n",
      "\n",
      "Racism example\n",
      "\n",
      " @dianh4 @exposefalsehood and it is muslims who were the first crusaders ,  attacking the christian world for centuries before it attacked back \n",
      "\n",
      "Sexism example\n",
      "\n",
      " rt @fruitondabottom: #feminismisequalitywhen men are actually listened to and part of the dialog. #heforshe #womenagainstfeminism http://t.…\n"
     ]
    }
   ],
   "source": [
    "data = \"twitter\"\n",
    "WORD = False\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "\n",
    "#Examine the labels\n",
    "print(\"Labels ['sexism','racism','none']\\n\\n\",np.array(labels[0:9]).transpose(),\"\\n\")\n",
    "\n",
    "# Examine the data\n",
    "print(\"None example\\n\\n\",x_text[0],\"\\n\")\n",
    "print(\"Racism example\\n\\n\", x_text[6],\"\\n\")\n",
    "print(\"Sexism example\\n\\n\",x_text[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using word based features\n",
      "Counter({2: 11036, 1: 3117, 0: 1937})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.803 (+/- 0.044)\n",
      "Recall Class 1 (avg): 0.744 (+/- 0.052)\n",
      "F1_score Class 1 (avg): 0.772 (+/- 0.037)\n",
      "Precision Class 2 (avg): 0.893 (+/- 0.023)\n",
      "Recall Class 2 (avg): 0.901 (+/- 0.018)\n",
      "F1_score Class 2 (avg): 0.897 (+/- 0.009)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.904 (+/- 0.035)\n",
      "Recall Class 1 (avg): 0.469 (+/- 0.056)\n",
      "F1_score Class 1 (avg): 0.617 (+/- 0.051)\n",
      "Precision Class 2 (avg): 0.806 (+/- 0.022)\n",
      "Recall Class 2 (avg): 0.963 (+/- 0.007)\n",
      "F1_score Class 2 (avg): 0.877 (+/- 0.011)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.832 (+/- 0.039)\n",
      "Recall Class 1 (avg): 0.663 (+/- 0.083)\n",
      "F1_score Class 1 (avg): 0.738 (+/- 0.062)\n",
      "Precision Class 2 (avg): 0.875 (+/- 0.026)\n",
      "Recall Class 2 (avg): 0.916 (+/- 0.012)\n",
      "F1_score Class 2 (avg): 0.895 (+/- 0.012)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.873 (+/- 0.044)\n",
      "Recall Class 1 (avg): 0.639 (+/- 0.068)\n",
      "F1_score Class 1 (avg): 0.737 (+/- 0.053)\n",
      "Precision Class 2 (avg): 0.865 (+/- 0.025)\n",
      "Recall Class 2 (avg): 0.937 (+/- 0.008)\n",
      "F1_score Class 2 (avg): 0.899 (+/- 0.013)\n"
     ]
    }
   ],
   "source": [
    "data = \"twitter\"\n",
    "WORD = True\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using char n-grams based features\n",
      "Counter({0: 102274, 1: 13590})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.591 (+/- 0.025)\n",
      "Recall Class 1 (avg): 0.823 (+/- 0.020)\n",
      "F1_score Class 1 (avg): 0.688 (+/- 0.017)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.839 (+/- 0.010)\n",
      "Recall Class 1 (avg): 0.554 (+/- 0.028)\n",
      "F1_score Class 1 (avg): 0.667 (+/- 0.021)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.602 (+/- 0.023)\n",
      "Recall Class 1 (avg): 0.845 (+/- 0.022)\n",
      "F1_score Class 1 (avg): 0.703 (+/- 0.017)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.887 (+/- 0.008)\n",
      "Recall Class 1 (avg): 0.547 (+/- 0.033)\n",
      "F1_score Class 1 (avg): 0.676 (+/- 0.025)\n"
     ]
    }
   ],
   "source": [
    "data = \"wiki\"\n",
    "WORD = False\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Using word based features\n",
      "Counter({0: 102274, 1: 13590})\n",
      "Model Type: svm\n",
      "Precision Class 1 (avg): 0.590 (+/- 0.022)\n",
      "Recall Class 1 (avg): 0.818 (+/- 0.026)\n",
      "F1_score Class 1 (avg): 0.686 (+/- 0.019)\n",
      "Model Type: naive\n",
      "Precision Class 1 (avg): 0.899 (+/- 0.016)\n",
      "Recall Class 1 (avg): 0.522 (+/- 0.036)\n",
      "F1_score Class 1 (avg): 0.660 (+/- 0.027)\n",
      "Model Type: lr\n",
      "Precision Class 1 (avg): 0.620 (+/- 0.027)\n",
      "Recall Class 1 (avg): 0.834 (+/- 0.023)\n",
      "F1_score Class 1 (avg): 0.711 (+/- 0.021)\n",
      "Model Type: random_forest\n",
      "Precision Class 1 (avg): 0.812 (+/- 0.022)\n",
      "Recall Class 1 (avg): 0.663 (+/- 0.027)\n",
      "F1_score Class 1 (avg): 0.730 (+/- 0.021)\n"
     ]
    }
   ],
   "source": [
    "data = \"wiki\"\n",
    "WORD = True\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
